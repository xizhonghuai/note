### åŸºæœ¬è‡ªç¼–ç å™¨

è€ƒè™‘ç›‘ç£å­¦ä¹ ç¥ç»ç½‘ç»œæ¨¡å‹ï¼š
$$
y = f_ğœƒ(x)
$$
xæ˜¯æ ·æœ¬æ•°æ®ï¼Œyæ˜¯é€šè¿‡xé¢„æµ‹å¾—åˆ°å®é™…å€¼ã€‚å¦‚æœä½¿ç”¨æ ·æœ¬ä¸­æ•°æ®xæ›¿ä»£æ ‡ç­¾æ•°æ®æ—¶æœ‰ï¼š
$$
x^`= f_ğœƒ(x)
$$
å³å°è¯•ç€åˆ©ç”¨æ•°æ®xæœ¬èº«ä½œä¸ºç›‘ç£ä¿¡å·æ¥æŒ‡å¯¼ç½‘ç»œçš„è®­ç»ƒï¼Œå¸Œæœ›ç¥ç»ç½‘ç»œèƒ½å¤Ÿå­¦ä¹ åˆ°æ˜ å°„ğ‘“ğœƒ:ğ’™ â†’ ğ’™ï¼ŒæŠŠç½‘ç»œğ‘“ğœƒåˆ‡åˆ†ä¸º 2 ä¸ªéƒ¨åˆ†ï¼Œå‰é¢çš„å­ç½‘ç»œå°è¯•å­¦ä¹ æ˜ å°„å…³ ç³»:ğ‘”ğœƒ1:ğ’™ â†’ ğ’›ï¼Œåé¢çš„å­ç½‘ç»œå°è¯•å­¦ä¹ æ˜ å°„å…³ç³»â„ğœƒ2:ğ’› â†’ ğ’™ï¼Œï¼ˆğ’›ç§°ä¸ºéšå˜é‡ï¼Œæ˜¯å¯¹ğ’™çš„é™ç»´æ“ä½œï¼Œä½“ç°äº†ğ’™çš„åˆ†å¸ƒï¼‰æˆ‘ä»¬æŠŠğ‘”ğœƒ1çœ‹æˆä¸€ä¸ªæ•°æ®ç¼–ç  (Encode)çš„è¿‡ç¨‹ï¼Œâ„ğœƒ2çœ‹æˆæ•°æ®è§£ç (Decode)çš„è¿‡ç¨‹ï¼Œç¼–ç å™¨å’Œè§£ç å™¨å…±åŒå®Œæˆäº†è¾“å…¥æ•°æ®ğ’™çš„ç¼–ç å’Œè§£ç è¿‡ç¨‹ï¼Œæˆ‘ä»¬æŠŠæ•´ä¸ªç½‘ç»œæ¨¡å‹ğ‘“ğœƒå«åšè‡ªåŠ¨ç¼–ç å™¨(Auto-Encoder)ï¼Œç®€ç§°è‡ªç¼–ç å™¨ã€‚

<img src="img\0.png" alt="0" style="zoom:80%;" />



<img src="img\1.png" alt="1" style="zoom:80%;" />
$$
x^` = h_ğœƒ(g_ğœƒ(x))  \quad\quad\quad\quad z=g_ğœƒ(x)
$$
 æ¨¡å‹æŸå¤±å‡½æ•°å¯é‡‡ç”¨æ–¹å·®è¡¨ç¤ºï¼š
$$
loss = \sum(x-x^`)^2
$$


è‡ªç¼–ç å™¨çš„æ¨¡å‹æ€§èƒ½ä¸€èˆ¬ä¸å¥½é‡åŒ–è¯„ä»·ï¼Œä½†æˆ‘ä»¬æœ€ç»ˆå¸Œæœ›è·å¾—è¿˜åŸåº¦è¾ƒé«˜çš„é‡å»ºæ ·æœ¬ã€‚ä¸€èˆ¬éœ€è¦æ ¹æ®å…·ä½“é—®é¢˜æ¥è®¨è®ºè‡ªç¼–ç å™¨çš„å­¦ä¹ æ•ˆæœï¼Œæ¯”å¦‚å¯¹äºå›¾ç‰‡é‡å»ºï¼Œä¸€èˆ¬ä¾èµ–äºäººå·¥ä¸»è§‚è¯„ä»·å›¾ç‰‡ç”Ÿæˆçš„è´¨é‡ã€‚

### MNIST å›¾ç‰‡é‡å»ºå®ä¾‹

```python
import numpy as np
import tensorflow as tf
import tensorflow.python.keras.datasets.mnist as mnist
import matplotlib.pyplot as plt

tf.compat.v1.disable_eager_execution()

TRAINING_STEPS = 10000
batch_size = 500


# å®šä¹‰mnistæ•°æ®åŠ è½½åˆå§‹åŒ–
def data_init():
    (x_train, y_train), (x_test, y_test) = mnist.load_data("E://python//tensorflow_study//MNIST_data//mnist.npz")
    x_train = np.reshape(x_train, (60000, 784))
    x_test = np.reshape(x_test, (10000, 784))

    x_train = (x_train - 255) / 255
    x_test = (x_test - 255) / 255

    # 5.5Wè®­ç»ƒé›†
    x_train_np = np.array(x_train[5000:], dtype='float32')

    # 1Wæµ‹è¯•é›†
    x_test_np = np.array(x_test[:], dtype='float32')

    return {"x_train": x_train_np, "x_test": x_test_np}


def getRandomIndex(n, x):
    # ç´¢å¼•èŒƒå›´ä¸º[0, n)ï¼Œéšæœºé€‰xä¸ªä¸é‡å¤ï¼Œæ³¨æ„replace=Falseä¸é‡å¤
    if x > n:
        x = n
    index = np.random.choice(np.arange(n), size=x, replace=False)
    return index


# éšæœºè·å–æ•°æ®
def data_batch_set(input_data_feed, feed_name):
    data_len = len(input_data_feed[feed_name])
    index = getRandomIndex(data_len, batch_size)
    return input_data_feed[feed_name][index]


# å®šä¹‰å±‚å‡½æ•°
def add_layer(input, in_size, out_size, active_function):
    # input è¾“å…¥çŸ©é˜µ
    # in_size è¾“å…¥çŸ©é˜µåˆ—å¤§å°
    # out_size è¾“å‡ºçŸ©é˜µåˆ—å¤§å°
    # active_function æ¿€æ´»å‡½æ•°
    weighs = tf.Variable(tf.compat.v1.random_normal([in_size, out_size]))
    # å®šä¹‰L2æ­£åˆ™åŒ–ï¼ˆå®šä¹‰åˆ»ç”»ç½‘ç»œå¤æ‚åº¦çš„æŸå¤±å‡½æ•°,è§£å†³è¿‡æ‹Ÿåˆé—®é¢˜ï¼‰
    regularizers_L2 = tf.keras.regularizers.l2(0.000005)
    weighs_loss = regularizers_L2(x=weighs)
    # åŠ å…¥é›†åˆ,lossesæ˜¯é›†åˆçš„åå­—ï¼Œç¬¬äºŒä¸ªå‚æ•°æ˜¯è¦åŠ å…¥è¿™ä¸ªé›†åˆçš„å†…å®¹ã€‚
    tf.compat.v1.add_to_collection('losses', weighs_loss)
    bais = tf.Variable(tf.compat.v1.random_normal([1, out_size]))
    # æ¿€åŠ±è¾“å…¥
    z_i = tf.matmul(input, weighs) + bais
    return active_function(z_i)


# å®šä¹‰å‰å‘ä¼ æ’­
def inference(input_tensor):
    # ç¼–ç å™¨éƒ¨åˆ†
    # ç¬¬ä¸€å±‚è¾“å…¥NX784å½¢çŠ¶è¾“å…¥ï¼Œè¾“å‡ºä¸ºNX512
    out_1 = add_layer(input=input_tensor, in_size=784, out_size=512, active_function=tf.nn.sigmoid)
    # ç¬¬äºŒå±‚è¾“å…¥ä¸ºNX512ï¼Œè¾“å‡ºä¸ºNX64
    out_2 = add_layer(input=out_1, in_size=512, out_size=64, active_function=tf.nn.sigmoid)
    # ç¬¬ä¸‰å±‚è¾“å…¥ä¸ºNX64ï¼Œè¾“å‡ºä¸ºNX16
    out_3 = add_layer(input=out_2, in_size=64, out_size=16, active_function=tf.nn.sigmoid)

    # è§£ç å™¨éƒ¨åˆ†
    # ç¬¬å››å±‚è¾“å…¥NX16å½¢çŠ¶è¾“å…¥ï¼Œè¾“å‡ºä¸ºNX64
    out_4 = add_layer(input=out_3, in_size=16, out_size=64, active_function=tf.nn.sigmoid)
    # ç¬¬äº”å±‚è¾“å…¥ä¸ºNX64ï¼Œè¾“å‡ºä¸ºNX512
    out_5 = add_layer(input=out_4, in_size=64, out_size=512, active_function=tf.nn.sigmoid)
    # ç¬¬å…­å±‚è¾“å…¥ä¸ºNX512ï¼Œè¾“å‡ºä¸ºNX784ï¼ˆæœ€åè¾“å‡ºå›¾ç‰‡ï¼‰
    out_6 = add_layer(input=out_5, in_size=512, out_size=784, active_function=tf.nn.sigmoid)

    return out_6


# å®šä¹‰æ¨¡å‹è®­ç»ƒè¿‡ç¨‹
def train():
    global_step = tf.Variable(0, trainable=False)

    # å®šä¹‰è¾“å…¥ã€æ ‡ç­¾ã€‚è¿™é‡Œæ ‡ç­¾y_å°±æ˜¯è¾“å…¥x
    x = tf.compat.v1.placeholder(tf.float32, [None, 784])
    y_ = tf.compat.v1.placeholder(tf.float32, [None, 784])

    # è®¡ç®—å½“å‰å‚æ•°åœ¨ç¥ç»ç½‘ç»œä¸Šçš„ç»“æœ
    y = inference(x)

    # å®šä¹‰æŸå¤±å‡½æ•°
    loss_mean = tf.reduce_mean(tf.reduce_sum(tf.square(y - y_)))

    # å°†loss_meanåŠ å…¥æŸå¤±é›†åˆã€‚
    tf.compat.v1.add_to_collection('losses', loss_mean)
    # æ€»æŸå¤±å‡½æ•°
    loss = tf.add_n(tf.compat.v1.get_collection('losses'))

    # åˆå§‹é€Ÿç‡0.1ï¼Œåé¢æ¯è®­ç»ƒ100æ¬¡ååœ¨å­¦ä¹ é€Ÿç‡åŸºç¡€ä¸Šä¹˜ä»¥0.96
    learning_rate = tf.compat.v1.train.exponential_decay(0.9999, global_step, 5000, 0.9, staircase=True)

    # ä½¿ç”¨tf.train.GradientDescentOptimizer ä¼˜åŒ–ç®—æ³•æ¥ä¼˜åŒ–æŸå¤±å‡½æ•°ã€‚
    train_step = tf.compat.v1.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)

    # åŠ è½½æ ·æœ¬æ•°æ®
    data_feed = data_init()
    # åˆå§‹åŒ–ä¼šè¯å¹¶å¼€å§‹è®­ç»ƒè¿‡ç¨‹ã€‚
    init_var = tf.compat.v1.global_variables_initializer()
    with tf.compat.v1.Session() as sess:
        sess.run(init_var)

        for i in range(TRAINING_STEPS):
            train_batch_x = data_batch_set(data_feed, feed_name='x_train')
            # æ ‡ç­¾å°±æ˜¯æ ·æœ¬x
            sess.run(train_step, feed_dict={x: train_batch_x, y_: train_batch_x})
            if i % 500 == 0:
                # loss
                loss_val = sess.run(loss, feed_dict={x: train_batch_x, y_: train_batch_x})
                print("After %d training step(s) , loss=%f" % (i, loss_val))

        # test
        test_batch_x = data_batch_set(data_feed, feed_name='x_test')
        # test_img
        test_x = test_batch_x[0]

        # åŸå›¾
        test_x_img = test_x * 255
        test_x_img = np.reshape(test_x_img, (28, 28))

        # é‡å»ºåçš„å›¾
        # è½¬æ¢ä¸º0-1ä¹‹é—´çš„å€¼
        test_y = tf.nn.sigmoid(y)
        test_y = sess.run(test_y, feed_dict={x: [test_x]})
        test_y_img = test_y * 255
        test_y_img = np.reshape(test_y_img, (28, 28))

        plt.subplot(1, 2, 1)
        plt.title('origin')
        plt.imshow(test_x_img)

        plt.subplot(1, 2, 2)
        plt.title('forecast')
        plt.imshow(test_y_img)

        plt.show()


def main():
    train()


if __name__ == '__main__':
    main()
```

<img src="E:\å­¦ä¹ èµ„æ–™\note\è‡ªç¼–ç å™¨\img\2.png" alt="2" style="zoom:80%;" />

### é™å™ªè‡ªç¼–ç å™¨

åŸºäºåŸºæœ¬çš„è‡ªç¼–ç æ¨¡å‹ï¼Œå°†è¾“å…¥æ ·æœ¬xåŠ ä¸Šå™ªå£°ï¼Œä½œä¸ºè¾“å…¥ï¼Œæ ‡ç­¾æ•°æ®ä½¿ç”¨åŸç”Ÿxå»è®­ç»ƒç¥ç»ç½‘ç»œçš„ä¸€ç§æ¨¡å‹ã€‚
$$
x = x + \epsilon  \quad\quad\quad\quad\epsilon -~ N(0,\sigma^2)
$$


<img src="img\3.png" alt="3" style="zoom:80%;" />