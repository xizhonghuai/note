[toc]



# 一、基本概念

## 1.1、数据集

### 1.1.1、定义

$$
D = \{D_1,D_2,...,D_i,...Dn\}
$$

$$
D_i = (\{x_{i1},x_{i2},x_{i3},...,x_{id}\},\{y_{i1},y_{i2},y_{i3},...,y_{im}\})=(\mathbf{x_i},\mathbf{y_i})
$$

在样本空间中采样N个示例(D~1~,D~2~,D~3~,...,D~n~)组成D，作为一个训练集。每个示例D~i~(或每个样本)由**x~i~**，**y~i~**两部分组成。
**x~i~**是一个d维向量，或称特征向量
**y~i~**是一个m维向量，或称标签

## 1.2、机器学习
### 1.2.1、定义

对某一任务，给定数据集D~train~，通过一种算法（学习算法），得到**x~i~**与**y~i~**的映射关系y=f(x)，映射关系**f**称为学习算法在数据集D上学习到的**经验(模型)**得到模型后并对未知数据D~test~进行预测。

> 机器学习形式化的定义
>
> 假设用P来评估计算机程序在某任务类T上的性能，若一个程序通过利用经验E在T中任务丰获得了性能改善，则我们就说关于T和P，该程序对E进行了学习

通过算法得到模型后，在另一数据集D~test~（D~test~、D~train~均来自同一样本空间采样）上进行预测的过程称为**测试**。我们希望模型能够很好适用于D~test~，这种在“新样本”上的适用能力称为**泛化**。

>具有强泛化能力的模型能很好地适用于整个样本空间.
>训练集应能能很好地反映出样本空间的特性.
>训练集中每个样本应独立地从样本空间分布上采样获得的，即独立同分布.
>训练样本越多，越能反映出样本空间特性，通过学习算法获得的模型强泛化能力越强



### 1.2.2、机器学习任务分类

根据训练数据集是否拥有标签信息,学习任务可分为：

**监督学习：**

$$
D_i = (\{x_{i1},x_{i2},x_{i3},...,x_{id}\},\{y_{i1},y_{i2},y_{i3},...,y_{im}\})=(\mathbf{x_i},\mathbf{y_i})
$$

**非监督学习：**
当数据集D中每个样本D~i~仅由x~i~组成，即不包含标签数据时
$$
D_i = \{x_{i1},x_{i2},x_{i3},...,x_{id}\}=\mathbf{x_i}
$$

根据模型预测输出值的情况，学习任务可分为：

**回归：**
输出值是连续的，如根据历史数据，预测未来十年全球人口数量

**分类:**
输出值是离散的，如根据邮件内容识别邮件是否是垃圾邮件（输出是一个逻辑状态，是或否，没有中间状态）




## 1.3、模型性能指标

### 1.3.1、 错误率
对于分类任务，如果有M个样本中，有a个样本分类错误，则模型的**误差率为a/m**。**精度为1-a/m**

### 1.3.2、误差

模型预测输出与真实输出的差异

### 1.3.3、经验误差

模型在训练集上的误差称为经验误差或训练误差

### 1.3.4、 泛化误差

模型在新样本上的误差称为**泛化误差**

**通常最终目的我们是希望模型的泛化误差越小越好**

在训练模型时为了得到泛化误差，以此来近似评价模型的好坏。通常准备两个数据集，一个是训练模型需要的训练集，另一个验证模型好坏的验证集。

训练集、验证集数据获取方法：

**留出法**：

将数据集D划分两个互斥集合D~train~(训练集) ，D~verify~（测试集）.  D~train~ ∩ D~verify~ = 空集,  D~train~ ∪ D~verify~ = D

在D~train~ 上训练出模型后，通过，D~verify~ 来评估模型的测试误差，作为泛化误差的估计。

 > 单次使用留出法得到的估计结果往往不够稳定可靠，在使用留出法时，一般要采用若干次随机划分、重复进行实验评估后取平均值作为留出法的评估结果.例如进行100次随机划分，每次产生一个训练/测试集用同时可得估计结呆的标于实验评估，100次后就得到100个结果?而留出法返回的则是这100个结果的平均.

**k折交叉验证法**:

将数据集D划分成K份，D= {D~1~，D~2~，...,D~k~}, 分别取其中一个子集作为测试集，其他剩余作为训练集。即将D分成了K组数据:

  > group~1~
  > D~train~= {D~1~，D~2~，...,D~k-1~}   D~verify~={D~k~}
  >
  > group~2~
  > D~train~= {D~1~，D~2~，...,D~k-2~} ∪ {D~k~}   D~verify~={D~k-1~}
  >
  > group~3~
  > D~train~= {D~1~，D~2~，...,D~k-3~} ∪ {D~k-1~,D~k~}   D~verify~={D~k-2~}
  >
  > ...
  >
  > group~k~
  > D~train~= {D~2~，...，D~k~}   D~verify~={D~1~}

获得k组训练/测试集后，可进行k次训练和测试，最终返回的是这k个测试结果的均值。

**自助法:**

在一个包含N个样本的数据集D中，每次随即抽取一个样本，并抽取N次，得到新的数据集D'.由于

每次采样可能采集到重复的样本，所以D中有部分样本没有出现在D‘中，假设每个样本不被抽中的概率为1-1/N，那么采样N次后，始终不被抽样中的概率为(1-1/N)^n^, 对N取极限则得到概率为
1/e  ≈ 0.368. 即有36.8%的样本没有出现在D’中。这样可以使用D‘作为训练集，D与D’的差集作为验证集。

  D~train~ = D‘

  D~verify~  = D - D’

自助法适用与当数据集比较少的情况。

  

**经验误差与泛化误差的关系：**

经验误差小，泛化误差可能会比较大（过拟合）

经验误差大，泛化误差可能大（欠拟合），可能小

**经验误差与泛化误差没有必然联系**

实际应在保证经验误差小（并不是越小越好）的情况下，减小泛化误差。

总之，最终目的是学得的模型应在样本空间中具有较强的泛化能力，即减小泛化误差

### 1.3.5、过拟合
学习算法过于强大，模型记住了训练样本中的**非普遍规律**，特点是经验误差很小，泛化误差大。 


<img src="img\1.png" alt="1" style="zoom:85%;" />

### 1.3.6、 欠拟合
学习算法过于弱小，模型不能表述样本普遍特性，特点是经验误差大，泛化误差大。

# 二、线性模型
## 2.1 线性模型一般形式
$$
y=f(x) = \omega_1 x_1+ \omega_2 x_2+ \omega_2 x_2+...+ \omega_d x_d+b
$$
或
$$
y=f(\pmb{x}) = \pmb{\omega}^T\pmb{x}  + b
$$

x（x~1~，x~2~，x~3~，...，x~d~）表示模型的输入，y表示输出。
ω（ω~1~，ω~2~，ω~3~，...，ω~d~）、b表示模型的参数，当ω、b参数确定后，模型也被确定下来。

给定数据集$D=\{D_1,D_2,...,D_n\}$中某个样本 $D_i = (\{x_{i1},x_{i2},x_{i3},...,x_{id}\},\{y_i\})$假设可以使用线性模型来表示数据集中**x与 y的关系**。
$$
\pmb{y} =  \pmb{\omega}^T\pmb{x}  + b
$$


## 2.2 单变量线性回归
考虑当输入特征仅只有一个的情况，即输入是一个标量。
设数据集$D =\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\}$
使用线性模型来表示数据集x、y的关系：
$$
y =f(x)= \omega x + b
$$
通过算法找到合适的ω、b参数，尽可能使得：
$$
y_i ≈ f(x_i) = \omega x_i + b
$$

使用均方误差来作为模型f的性能度量
$$
E = \sum_{i=1}^n (\omega x_i + b-y_i)^2=\sum_{i=1}^n (y'_i-y_i)^2 
$$
>y~i~表示数据集中样本标记，y'~i~表示模型f预测的实际输出。

目标函数E的优化过程：
对于函数E，当取得极小值时，对应的ω、b就是最优解。
对b求导：
$$
\frac{\partial E}{\partial b} = \frac{\partial \sum_{i=1}^n (\omega x_i + b-y_i)^2}{\partial b}
$$
$$
\frac{\partial E}{\partial b} = 2\sum_{i=1}^n (\omega x_i + b-y_i)
$$
令$\frac{\partial E}{\partial b}=0$，则：
$$
\sum_{i=1}^n (\omega x_i + b-y_i)=0
$$
$$
\sum_{i=1}^n \omega x_i + \sum_{i=1}^n b - \sum_{i=1}^n y_i=0
$$
$$
\omega \sum_{i=1}^n x_i + \sum_{i=1}^n b - \sum_{i=1}^n y_i=0
$$

对于随机变量a有 $\sum_{i=1}^n a_i = N\overline{a}$，$\overline{a} 表示a的平均数$，所以对上式表示为:
$$
\omega N\overline{x} + Nb = N\overline{y}
$$

**b的最优解**
$$
b = \overline{y} - \omega \overline{x}
$$

接下来对ω求导：
$$
\frac{\partial E}{\partial \omega} = \frac{\partial \sum_{i=1}^n (\omega x_i + b-y_i)^2}{\partial \omega}
$$
$$
\frac{\partial E}{\partial \omega} =   2\sum_{i=1}^n (\omega x_i + b-y_i)x_i
$$

令$\frac{\partial E}{\partial \omega}=0$，则：
$$
\sum_{i=1}^n (\omega x_i + b-y_i)x_i = 0
$$
$$
\sum_{i=1}^n \omega x_i^2 + \sum_{i=1}^n bx_i- \sum_{i=1}^n y_ix_i = 0
$$
$$
\omega N\overline{x^2} + bN\overline{x}- N\overline{xy} = 0
$$
将b的最优解带入上式:
$$
\omega N\overline{x^2} + (\overline{y} - \omega \overline{x})N \ \overline{x}- N\overline{xy} = 0
$$
$$
\omega \overline{x^2} + (\overline{y} - \omega \overline{x})\overline{x}- \overline{xy} = 0
$$
$$
\omega \overline{x^2}  - \omega \overline{x}^2 + \overline{x} \ \overline{y} - \overline{xy} = 0
$$
$$
\omega (\overline{x^2}  -  \overline{x}^2)  = \overline{xy} - \overline{x} \ \overline{y}
$$

**ω的最优解:**
$$
\omega =  \frac{\overline{xy} - \overline{x} \ \overline{y}}{\overline{x^2}  -  \overline{x}^2}
$$


## 2.3 多元线性回归

给定数据集$D = \{D_1,D_2,...,D_i,...Dn\}$,其中
$$
D_i = (\{x_{i1},x_{i2},x_{i3},...,x_{id}\},\{y_i\})
$$

使用线性模型来表示数据集x、y的关系：
$$
y=f(\pmb{x}) = \pmb{\omega}^T\pmb{x}  + b
$$

通过算法找到合适的ω、b参数，尽可能使得：
$$
y_i≈f(\pmb{x_i}) = \pmb{\omega}^T\pmb{x_i}  + b  \quad (\pmb{x_i}\in R^d,y_i \in R)
$$
模型的目标优化函数使用均方误差表示：
$$
E = \sum_{i=1}^n (y'_i-y_i)^2 = \sum_{i=1}^n (\pmb{\omega}^T\pmb{x_i}  + b -y_i)^2
$$

为了达到最小化E的目的，可通过矩阵方式求解ω、b的最优解：

构造矩阵$\pmb{\omega}$：
$$
\pmb{\omega} = \begin{bmatrix} b \\ \omega_1\\ \omega_2 \\...\\ \omega_d \end{bmatrix}
$$


构造矩阵$\pmb{x}$：
$$
\pmb{x} = \begin{bmatrix} 1&x_{11}&x_{12}&x_{13}&...&x_{1d} \\ 
1&x_{21}&x_{22}&x_{23}&...&x_{2d} \\ 
1&x_{31}&x_{32}&x_{33}&...&x_{3d} \\ 
.&.&.&.&...&. \\ 
1&x_{m1}&x_{m2}&x_{m3}&...&x_{md} \\ 
.&.&.&.&...&. \\ 
1&x_{n1}&x_{n2}&x_{n3}&...&x_{nd} \\ 


\end{bmatrix}
$$

构造矩阵$\pmb{y'}$：
$$
\pmb{y'} = \begin{bmatrix} y'_1 \\ y'_2 \\ y'_3 \\ ... \\ y'_n
\end{bmatrix}
$$
因为:
$$
\pmb{x}\pmb{\omega} = \begin{bmatrix} b+\omega_1 x_{11}+\omega_2 x_{12}+...+\omega_d x_{1d}
\\ b+\omega_1 x_{21}+\omega_2 x_{22}+...+\omega_d x_{2d}
\\ b+\omega_1 x_{31}+\omega_2 x_{32}+...+\omega_d x_{3d} 
\\ ... 
\\ b+\omega_1 x_{n1}+\omega_2 x_{n2}+...+\omega_d x_{nd}
\end{bmatrix}
$$

所以显然有：
$$
\pmb{y'} = \pmb{x}\pmb{\omega}
$$

将数据集中所以样本的标签用矩阵表示为:
$$
\pmb{y} = \begin{bmatrix} y_1 \\ y_2 \\ y_3 \\ ... \\ y_n\end{bmatrix}
$$
所以目标函数E可表示为：
$$
E = \sum_{i=1}^n (\omega x_i + b-y_i)^2=\sum_{i=1}^n (y'_i-y_i)^2=||\pmb{x}\pmb{\omega} - \pmb{y}||_2^2 
 \quad (|| \pmb a ||_2^2 = a_1^2+a_2^2+a_3^2+...+a_n^2)
$$


因为对于向量a有：
$$
||\pmb a||_2^2 = \pmb a^T \pmb a
$$
所以：
$$
E = ||\pmb{x}\pmb{\omega} - \pmb{y}||_2^2 =(\pmb{x}\pmb{\omega} - \pmb{y})^T(\pmb{x}\pmb{\omega} - \pmb{y})
$$

$$
E = (\pmb{x}\pmb{\omega})^T(\pmb{x}\pmb{\omega}) - (\pmb{x}\pmb{\omega})^T\pmb{y} - \pmb{y}^T(\pmb{x}\pmb{\omega})+\pmb{y}^T\pmb{y}
$$

$$
E = (\pmb{\omega}^T\pmb{x}^T)(\pmb{x}\pmb{\omega}) - (\pmb{\omega}^T\pmb{x}^T)\pmb{y} - \pmb{y}^T(\pmb{x}\pmb{\omega})+\pmb{y}^T\pmb{y}
$$

因为$((\pmb{\omega}^T\pmb{x}^T)\pmb{y})^T = \pmb{y}^T(\pmb{x}\pmb{\omega})$,且$(\pmb{\omega}^T\pmb{x}^T)\pmb{y} \quad、\pmb{y}^T(\pmb{x}\pmb{\omega})$ 均是一个标量，所有两项的值相等，所以上式为：
$$
E = (\pmb{\omega}^T\pmb{x}^T)(\pmb{x}\pmb{\omega}) - 2(\pmb{\omega}^T\pmb{x}^T)\pmb{y} +\pmb{y}^T\pmb{y}
$$
对ω求导得：
$$
\frac{\partial E}{\partial \omega} = 2 \pmb x^T \pmb x \pmb{\omega} - 2\pmb x^T \pmb y
$$


令$\frac{\partial E}{\partial \omega}=0$，则：
$$
\pmb x^T \pmb x \pmb{\omega} = \pmb x^T \pmb y
$$
假设矩阵$\pmb x^T \pmb x$ 的逆矩阵存在，且为$\pmb A$，则在等式两边同时左乘$\pmb A$后得到$\pmb \omega$的最优解：
$$
\pmb{\omega} =\pmb A \pmb x^T \pmb y   \quad (\pmb A = (\pmb x^T \pmb x)^{-1})
$$
最终多元线性回归模型可表示为：
$$
f(\pmb{x_i}) = \pmb{x_i} \pmb{\omega}= \pmb{x_i}(\pmb A \pmb x^T \pmb y) \quad (\pmb{ x_i}\in R^{d+1} ，\pmb{ x_i}=\{1,x_{i1},x_{i2},x_{i3},...,x_{id}\})
$$



## 2.4 GDP增长率与三大产业增长率关系

```python
import matplotlib.pyplot as plt
import numpy as np


def read_data():
    # data2.txt 数据结构说明
    # GDP增长率/第一产业增长率/第二产业增长率/第三产业增长率
    # 数据来源:http://www.gov.cn/shuju/hgjjyxqk/detail.html?q=0
    # 共18个样本
    data = np.loadtxt("data2.txt")
    # 打乱顺序
    state = np.random.get_state()
    np.random.set_state(state)
    np.random.shuffle(data)

    # 训练样本个数
    train_specimen_number = 15
    train_data = data[0:train_specimen_number]
    test_data = data[train_specimen_number:]
    train_data_x_y = np.split(train_data, [3], axis=1)
    test_data_x_y = np.split(test_data, [3], axis=1)
    return {"train_data": train_data_x_y
        , "test_data": test_data_x_y}


def merge_data(data_feed):
    train_data = data_feed["train_data"]
    test_data = data_feed["test_data"]
    x = np.vstack((train_data[0], test_data[0]))  # 沿着矩阵行拼接
    y = np.vstack((train_data[1], test_data[1]))  # 沿着矩阵行拼接
    return x, y


def scatter(data_feed):
    """
    样本数据散点图
    :param data_feed:
    :return:
    """
    plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签
    plt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号
    x, y = merge_data(data_feed)

    # 各变量与GDP增长率相关系数
    print("相关系数")
    corrc1 = np.corrcoef(x[:, 0], y[:, 0])[0, 1]
    print("corrc1 = %.2f" % corrc1)
    corrc2 = np.corrcoef(x[:, 1], y[:, 0])[0, 1]
    print("corrc2 = %.2f" % corrc2)
    corrc3 = np.corrcoef(x[:, 2], y[:, 0])[0, 1]
    print("corrc2 = %.2f" % corrc3)
    # 相关显著性检验 n=18-2,a=0.001,查表临界值=0.708,corrc>0.708

    # 绘制散点图
    s1 = plt.scatter(x[:, 0], y)
    s2 = plt.scatter(x[:, 1], y)
    s3 = plt.scatter(x[:, 2], y)
    plt.xlabel('产业增长率')
    plt.ylabel('GDP增长率')
    plt.legend((s1, s2, s3), ('第一产业', '第二产业', '第三产业'))
    plt.show()


def get_params(train_data_x_y):
    """
    计算参数w
    w = ((x^Tx)^-1)x^Ty
    :param train_data_x_y:
    :return:w
    """
    # x_shape=(train_specimen_number,d+1)
    x = train_data_x_y[0]
    x = np.hstack((np.ones((x.shape[0], 1)), x))  # 沿着矩阵列拼接
    # y_shape=(train_specimen_number,1)
    y = train_data_x_y[1]
    # x_transpose_shape=(d+1,train_specimen_number)
    x_transpose = x.T
    x_ = np.matmul(x_transpose, x)
    # 计算x_的逆矩阵(如果存在)
    # x_inv_shape=(d+1,d+1)
    x_inv = np.linalg.inv(x_)
    # w_shape=(d+1,1)
    w = np.matmul(x_inv, np.matmul(x_transpose, y))
    print("回归系数:")
    print(w)
    return w


def active_function(z_array):
    return z_array


def model(w_params, specimen_x):
    """
    定义线性模型
    y=w0+w1*x1+w2*x3+w3*x4
    :param specimen_x:
    :return:
    """
    x = np.hstack((np.ones((specimen_x.shape[0], 1)), specimen_x))  # 沿着矩阵列拼接
    return active_function(np.matmul(x, w_params))


def rmse(predictions, targets):
    return np.sqrt(((predictions - targets) ** 2).mean())


def validate_model(w_params, data_feed):
    """
    验证模型
    :param specimen_x_y:
    :return:
    """
    train_data = data_feed["train_data"]
    test_data = data_feed["test_data"]
    train_x = train_data[0]
    train_y = train_data[1]
    test_x = test_data[0]
    test_y = test_data[1]

    # 计算训练误差
    train_y_ = model(w_params, train_x)
    train_err = rmse(train_y_, train_y)
    print("train_err = %.8f" % train_err)

    # 计算泛化误差
    test_y_ = model(w_params, test_x)
    test_err = rmse(test_y_, test_y)
    print("test_err = %.8f" % test_err)

    x, y = merge_data(data_feed)
    y_ = model(w_params, x)
    plt.plot(y_)
    plt.plot(y)
    plt.legend(["预测值", "实际值"])
    plt.show()


if __name__ == '__main__':
    data_feed = read_data()
    scatter(data_feed)
    w_params = get_params(data_feed["train_data"])
    validate_model(w_params, data_feed)
```

<img src="img\3.png" alt="3" style="zoom:80%;" /><img src="img\4.png" alt="4" style="zoom:80%;" />



