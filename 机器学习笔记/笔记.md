[toc]



# 一、基本概念

## 1.1、数据集

### 1.1.1、定义

$$
D = \{D_1,D_2,...,D_i,...Dn\}
$$

$$
D_i = (\{x_{i1},x_{i2},x_{i3},...,x_{id}\},\{y_{i1},y_{i2},y_{i3},...,y_{im}\})=(\mathbf{x_i},\mathbf{y_i})
$$

在样本空间中采样N个示例(D~1~,D~2~,D~3~,...,D~n~)组成D，作为一个训练集。每个示例D~i~(或每个样本)由**x~i~**，**y~i~**两部分组成。
**x~i~**是一个d维向量，或称特征向量
**y~i~**是一个m维向量，或称标签

## 1.2、机器学习
### 1.2.1、定义

对某一任务，给定数据集D~train~，通过一种算法（学习算法），得到**x~i~**与**y~i~**的映射关系y=f(x)，映射关系**f**称为学习算法在数据集D上学习到的**经验(模型)**得到模型后并对未知数据D~test~进行预测。

> 机器学习形式化的定义
>
> 假设用P来评估计算机程序在某任务类T上的性能，若一个程序通过利用经验E在T中任务丰获得了性能改善，则我们就说关于T和P，该程序对E进行了学习

通过算法得到模型后，在另一数据集D~test~（D~test~、D~train~均来自同一样本空间采样）上进行预测的过程称为**测试**。我们希望模型能够很好适用于D~test~，这种在“新样本”上的适用能力称为**泛化**。

>具有强泛化能力的模型能很好地适用于整个样本空间.
>训练集应能能很好地反映出样本空间的特性.
>训练集中每个样本应独立地从样本空间分布上采样获得的，即独立同分布.
>训练样本越多，越能反映出样本空间特性，通过学习算法获得的模型强泛化能力越强



### 1.2.2、机器学习任务分类

根据训练数据集是否拥有标签信息,学习任务可分为：

**监督学习：**

$$
D_i = (\{x_{i1},x_{i2},x_{i3},...,x_{id}\},\{y_{i1},y_{i2},y_{i3},...,y_{im}\})=(\mathbf{x_i},\mathbf{y_i})
$$

**非监督学习：**
当数据集D中每个样本D~i~仅由x~i~组成，即不包含标签数据时
$$
D_i = \{x_{i1},x_{i2},x_{i3},...,x_{id}\}=\mathbf{x_i}
$$

根据模型预测输出值的情况，学习任务可分为：

**回归：**
输出值是连续的，如根据历史数据，预测未来十年全球人口数量

**分类:**
输出值是离散的，如根据邮件内容识别邮件是否是垃圾邮件（输出是一个逻辑状态，是或否，没有中间状态）




## 1.3、模型性能指标

### 1.3.1、 错误率
对于分类任务，如果有M个样本中，有a个样本分类错误，则模型的**误差率为a/m**。**精度为1-a/m**

### 1.3.2、误差

模型预测输出与真实输出的差异

### 1.3.3、经验误差

模型在训练集上的误差称为经验误差或训练误差

### 1.3.4、 泛化误差

模型在新样本上的误差称为**泛化误差**

**通常最终目的我们是希望模型的泛化误差越小越好**

在训练模型时为了得到泛化误差，以此来近似评价模型的好坏。通常准备两个数据集，一个是训练模型需要的训练集，另一个验证模型好坏的验证集。

训练集、验证集数据获取方法：

**留出法**：

将数据集D划分两个互斥集合D~train~(训练集) ，D~verify~（测试集）.  D~train~ ∩ D~verify~ = 空集,  D~train~ ∪ D~verify~ = D

在D~train~ 上训练出模型后，通过，D~verify~ 来评估模型的测试误差，作为泛化误差的估计。

 > 单次使用留出法得到的估计结果往往不够稳定可靠，在使用留出法时，一般要采用若干次随机划分、重复进行实验评估后取平均值作为留出法的评估结果.例如进行100次随机划分，每次产生一个训练/测试集用同时可得估计结呆的标于实验评估，100次后就得到100个结果?而留出法返回的则是这100个结果的平均.

**k折交叉验证法**:

将数据集D划分成K份，D= {D~1~，D~2~，...,D~k~}, 分别取其中一个子集作为测试集，其他剩余作为训练集。即将D分成了K组数据:

  > group~1~
  > D~train~= {D~1~，D~2~，...,D~k-1~}   D~verify~={D~k~}
  >
  > group~2~
  > D~train~= {D~1~，D~2~，...,D~k-2~} ∪ {D~k~}   D~verify~={D~k-1~}
  >
  > group~3~
  > D~train~= {D~1~，D~2~，...,D~k-3~} ∪ {D~k-1~,D~k~}   D~verify~={D~k-2~}
  >
  > ...
  >
  > group~k~
  > D~train~= {D~2~，...，D~k~}   D~verify~={D~1~}

获得k组训练/测试集后，可进行k次训练和测试，最终返回的是这k个测试结果的均值。

**自助法:**

在一个包含N个样本的数据集D中，每次随即抽取一个样本，并抽取N次，得到新的数据集D'.由于

每次采样可能采集到重复的样本，所以D中有部分样本没有出现在D‘中，假设每个样本不被抽中的概率为1-1/N，那么采样N次后，始终不被抽样中的概率为(1-1/N)^n^, 对N取极限则得到概率为
1/e  ≈ 0.368. 即有36.8%的样本没有出现在D’中。这样可以使用D‘作为训练集，D与D’的差集作为验证集。

  D~train~ = D‘

  D~verify~  = D - D’

自助法适用与当数据集比较少的情况。

  

**经验误差与泛化误差的关系：**

经验误差小，泛化误差可能会比较大（过拟合）

经验误差大，泛化误差可能大（欠拟合），可能小

**经验误差与泛化误差没有必然联系**

实际应在保证经验误差小（并不是越小越好）的情况下，减小泛化误差。

总之，最终目的是学得的模型应在样本空间中具有较强的泛化能力，即减小泛化误差

### 1.3.5、过拟合
学习算法过于强大，模型记住了训练样本中的**非普遍规律**，特点是经验误差很小，泛化误差大。 


<img src="img\1.png" alt="1" style="zoom:85%;" />

### 1.3.6、 欠拟合
学习算法过于弱小，模型不能表述样本普遍特性，特点是经验误差大，泛化误差大。

# 二、线性模型
## 2.1 线性模型一般形式
$$
y=f(x) = \omega_1 x_1+ \omega_2 x_2+ \omega_2 x_2+...+ \omega_d x_d+b
$$
或
$$
y=f(\pmb{x}) = \pmb{\omega}^T\pmb{x}  + b
$$

x（x~1~，x~2~，x~3~，...，x~d~）表示模型的输入，y表示输出。
ω（ω~1~，ω~2~，ω~3~，...，ω~d~）、b表示模型的参数，当ω、b参数确定后，模型也被确定下来。

给定数据集$D=\{D_1,D_2,...,D_n\}$中某个样本 $D_i = (\{x_{i1},x_{i2},x_{i3},...,x_{id}\},\{y_i\})$假设可以使用线性模型来表示数据集中**x与 y的关系**。
$$
\pmb{y} =  \pmb{\omega}^T\pmb{x}  + b
$$


## 2.2 单变量线性回归
考虑当输入特征仅只有一个的情况，即输入是一个标量。
设数据集$D =\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\}$
使用线性模型来表示数据集x、y的关系：
$$
y =f(x)= \omega x + b
$$
通过算法找到合适的ω、b参数，尽可能使得：
$$
y_i ≈ f(x_i) = \omega x_i + b
$$

使用均方误差来作为模型f的性能度量
$$
E = \sum_{i=1}^n (\omega x_i + b-y_i)^2=\sum_{i=1}^n (y'_i-y_i)^2
$$
>y~i~表示数据集中样本标记，y'~i~表示模型f预测的实际输出。

目标函数E的优化过程：
对于函数E，当取得极小值时，对应的ω、b就是最优解。
对b求导：
$$
\frac{\partial E}{\partial b} = \frac{\partial \sum_{i=1}^n (\omega x_i + b-y_i)^2}{\partial b}
$$
$$
\frac{\partial E}{\partial b} = 2\sum_{i=1}^n (\omega x_i + b-y_i)
$$
令$\frac{\partial E}{\partial b}=0$，则：
$$
\sum_{i=1}^n (\omega x_i + b-y_i)=0
$$
$$
\sum_{i=1}^n \omega x_i + \sum_{i=1}^n b - \sum_{i=1}^n y_i=0
$$
$$
\omega \sum_{i=1}^n x_i + \sum_{i=1}^n b - \sum_{i=1}^n y_i=0
$$

对于随机变量a有 $\sum_{i=1}^n a_i = N\overline{a}$，$\overline{a} 表示a的平均数$，所以对上式表示为:
$$
\omega N\overline{x} + Nb = N\overline{y}
$$

**b的最优解**
$$
b = \overline{y} - \omega \overline{x}
$$

接下来对ω求导：
$$
\frac{\partial E}{\partial \omega} = \frac{\partial \sum_{i=1}^n (\omega x_i + b-y_i)^2}{\partial \omega}
$$
$$
\frac{\partial E}{\partial \omega} =   2\sum_{i=1}^n (\omega x_i + b-y_i)x_i
$$

令$\frac{\partial E}{\partial \omega}=0$，则：
$$
\sum_{i=1}^n (\omega x_i + b-y_i)x_i = 0
$$
$$
\sum_{i=1}^n \omega x_i^2 + \sum_{i=1}^n bx_i- \sum_{i=1}^n y_ix_i = 0
$$
$$
\omega N\overline{x^2} + bN\overline{x}- N\overline{xy} = 0
$$
将b的最优解带入上式:
$$
\omega N\overline{x^2} + (\overline{y} - \omega \overline{x})N \ \overline{x}- N\overline{xy} = 0
$$
$$
\omega \overline{x^2} + (\overline{y} - \omega \overline{x})\overline{x}- \overline{xy} = 0
$$
$$
\omega \overline{x^2}  - \omega \overline{x}^2 + \overline{x} \ \overline{y} - \overline{xy} = 0
$$
$$
\omega (\overline{x^2}  -  \overline{x}^2)  = \overline{xy} - \overline{x} \ \overline{y}
$$

**ω的最优解:**
$$
\omega =  \frac{\overline{xy} - \overline{x} \ \overline{y}}{\overline{x^2}  -  \overline{x}^2}
$$


## 2.3 多元线性回归

给定数据集$D = \{D_1,D_2,...,D_i,...Dn\}$,其中
$$
D_i = (\{x_{i1},x_{i2},x_{i3},...,x_{id}\},\{y_i\})
$$

使用线性模型来表示数据集x、y的关系：
$$
y=f(\pmb{x}) = \pmb{\omega}^T\pmb{x}  + b
$$

通过算法找到合适的ω、b参数，尽可能使得：
$$
y_i≈f(\pmb{x_i}) = \pmb{\omega}^T\pmb{x_i}  + b  \quad (\pmb{x_i}\in R^d,y_i \in R)
$$
模型的目标优化函数使用均方误差表示：
$$
E = \sum_{i=1}^n (y'_i-y_i)^2 = \sum_{i=1}^n (\pmb{\omega}^T\pmb{x_i}  + b -y_i)^2
$$

为了达到最小化E的目的，可通过矩阵方式求解ω、b的最优解：

构造矩阵$\pmb{\omega}$：
$$
\pmb{\omega} = \begin{bmatrix} b \\ \omega_1\\ \omega_2 \\...\\ \omega_d \end{bmatrix}
$$


构造矩阵$\pmb{x}$：
$$
\pmb{x} = \begin{bmatrix} 1&x_{11}&x_{12}&x_{13}&...&x_{1d} \\ 
1&x_{21}&x_{22}&x_{23}&...&x_{2d} \\ 
1&x_{31}&x_{32}&x_{33}&...&x_{3d} \\ 
.&.&.&.&...&. \\ 
1&x_{m1}&x_{m2}&x_{m3}&...&x_{md} \\ 
.&.&.&.&...&. \\ 
1&x_{n1}&x_{n2}&x_{n3}&...&x_{nd} \\ 


\end{bmatrix}
$$

构造矩阵$\pmb{y'}$：
$$
\pmb{y'} = \begin{bmatrix} y'_1 \\ y'_2 \\ y'_3 \\ ... \\ y'_n
\end{bmatrix}
$$
因为:
$$
\pmb{x}\pmb{\omega} = \begin{bmatrix} b+\omega_1 x_{11}+\omega_2 x_{12}+...+\omega_d x_{1d}
\\ b+\omega_1 x_{21}+\omega_2 x_{22}+...+\omega_d x_{2d}
\\ b+\omega_1 x_{31}+\omega_2 x_{32}+...+\omega_d x_{3d} 
\\ ... 
\\ b+\omega_1 x_{n1}+\omega_2 x_{n2}+...+\omega_d x_{nd}
\end{bmatrix}
$$

所以显然有：
$$
\pmb{y'} = \pmb{x}\pmb{\omega}
$$

将数据集中所以样本的标签用矩阵表示为:
$$
\pmb{y} = \begin{bmatrix} y_1 \\ y_2 \\ y_3 \\ ... \\ y_n\end{bmatrix}
$$
所以目标函数E可表示为：
$$
E = \sum_{i=1}^n (\omega x_i + b-y_i)^2=\sum_{i=1}^n (y'_i-y_i)^2=||\pmb{x}\pmb{\omega} - \pmb{y}||_2^2 
 \quad (|| \pmb a ||_2^2 = a_1^2+a_2^2+a_3^2+...+a_n^2)
$$


因为对于向量a有：
$$
||\pmb a||_2^2 = \pmb a^T \pmb a
$$
所以：
$$
E = ||\pmb{x}\pmb{\omega} - \pmb{y}||_2^2 =(\pmb{x}\pmb{\omega} - \pmb{y})^T(\pmb{x}\pmb{\omega} - \pmb{y})
$$

$$
E = (\pmb{x}\pmb{\omega})^T(\pmb{x}\pmb{\omega}) - (\pmb{x}\pmb{\omega})^T\pmb{y} - \pmb{y}^T(\pmb{x}\pmb{\omega})+\pmb{y}^T\pmb{y}
$$

$$
E = (\pmb{\omega}^T\pmb{x}^T)(\pmb{x}\pmb{\omega}) - (\pmb{\omega}^T\pmb{x}^T)\pmb{y} - \pmb{y}^T(\pmb{x}\pmb{\omega})+\pmb{y}^T\pmb{y}
$$

因为$((\pmb{\omega}^T\pmb{x}^T)\pmb{y})^T = \pmb{y}^T(\pmb{x}\pmb{\omega})$,且$(\pmb{\omega}^T\pmb{x}^T)\pmb{y} \quad、\pmb{y}^T(\pmb{x}\pmb{\omega})$ 均是一个标量，所有两项的值相等，所以上式为：
$$
E = (\pmb{\omega}^T\pmb{x}^T)(\pmb{x}\pmb{\omega}) - 2(\pmb{\omega}^T\pmb{x}^T)\pmb{y} +\pmb{y}^T\pmb{y}
$$
对ω求导得：
$$
\frac{\partial E}{\partial \omega} = 2 \pmb x^T \pmb x \pmb{\omega} - 2\pmb x^T \pmb y
$$


令$\frac{\partial E}{\partial \omega}=0$，则：
$$
\pmb x^T \pmb x \pmb{\omega} = \pmb x^T \pmb y
$$
假设矩阵$\pmb x^T \pmb x$ 的逆矩阵存在，且为$\pmb A$，则在等式两边同时左乘$\pmb A$后得到$\pmb \omega$的最优解：
$$
\pmb{\omega} =\pmb A \pmb x^T \pmb y   \quad (\pmb A = (\pmb x^T \pmb x)^{-1})
$$
最终多元线性回归模型可表示为：
$$
f(\pmb{x_i}) = \pmb{x_i} \pmb{\omega}= \pmb{x_i}(\pmb A \pmb x^T \pmb y) \quad (\pmb{ x_i}\in R^{d+1} ，\pmb{ x_i}=\{1,x_{i1},x_{i2},x_{i3},...,x_{id}\})
$$



## 2.4 GDP增长率与三大产业增长率关系

```python
import matplotlib.pyplot as plt
import numpy as np


def read_data():
    # data2.txt 数据结构说明
    # GDP增长率/第一产业增长率/第二产业增长率/第三产业增长率
    # 数据来源:http://www.gov.cn/shuju/hgjjyxqk/detail.html?q=0
    # 共18个样本
    data = np.loadtxt("data2.txt")
    # 打乱顺序
    state = np.random.get_state()
    np.random.set_state(state)
    np.random.shuffle(data)

    # 训练样本个数
    train_specimen_number = 15
    train_data = data[0:train_specimen_number]
    test_data = data[train_specimen_number:]
    train_data_x_y = np.split(train_data, [3], axis=1)
    test_data_x_y = np.split(test_data, [3], axis=1)
    return {"train_data": train_data_x_y
        , "test_data": test_data_x_y}


def merge_data(data_feed):
    train_data = data_feed["train_data"]
    test_data = data_feed["test_data"]
    x = np.vstack((train_data[0], test_data[0]))  # 沿着矩阵行拼接
    y = np.vstack((train_data[1], test_data[1]))  # 沿着矩阵行拼接
    return x, y


def scatter(data_feed):
    """
    样本数据散点图
    :param data_feed:
    :return:
    """
    plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签
    plt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号
    x, y = merge_data(data_feed)

    # 各变量与GDP增长率相关系数
    print("相关系数")
    corrc1 = np.corrcoef(x[:, 0], y[:, 0])[0, 1]
    print("corrc1 = %.2f" % corrc1)
    corrc2 = np.corrcoef(x[:, 1], y[:, 0])[0, 1]
    print("corrc2 = %.2f" % corrc2)
    corrc3 = np.corrcoef(x[:, 2], y[:, 0])[0, 1]
    print("corrc2 = %.2f" % corrc3)
    # 相关显著性检验 n=18-2,a=0.001,查表临界值=0.708,corrc>0.708

    # 绘制散点图
    s1 = plt.scatter(x[:, 0], y)
    s2 = plt.scatter(x[:, 1], y)
    s3 = plt.scatter(x[:, 2], y)
    plt.xlabel('产业增长率')
    plt.ylabel('GDP增长率')
    plt.legend((s1, s2, s3), ('第一产业', '第二产业', '第三产业'))
    plt.show()


def get_params(train_data_x_y):
    """
    计算参数w
    w = ((x^Tx)^-1)x^Ty
    :param train_data_x_y:
    :return:w
    """
    # x_shape=(train_specimen_number,d+1)
    x = train_data_x_y[0]
    x = np.hstack((np.ones((x.shape[0], 1)), x))  # 沿着矩阵列拼接
    # y_shape=(train_specimen_number,1)
    y = train_data_x_y[1]
    # x_transpose_shape=(d+1,train_specimen_number)
    x_transpose = x.T
    x_ = np.matmul(x_transpose, x)
    # 计算x_的逆矩阵(如果存在)
    # x_inv_shape=(d+1,d+1)
    x_inv = np.linalg.inv(x_)
    # w_shape=(d+1,1)
    w = np.matmul(x_inv, np.matmul(x_transpose, y))
    print("回归系数:")
    print(w)
    return w


def active_function(z_array):
    return z_array


def model(w_params, specimen_x):
    """
    定义线性模型
    y=w0+w1*x1+w2*x3+w3*x4
    :param specimen_x:
    :return:
    """
    x = np.hstack((np.ones((specimen_x.shape[0], 1)), specimen_x))  # 沿着矩阵列拼接
    return active_function(np.matmul(x, w_params))


def rmse(predictions, targets):
    return np.sqrt(((predictions - targets) ** 2).mean())


def validate_model(w_params, data_feed):
    """
    验证模型
    :param specimen_x_y:
    :return:
    """
    train_data = data_feed["train_data"]
    test_data = data_feed["test_data"]
    train_x = train_data[0]
    train_y = train_data[1]
    test_x = test_data[0]
    test_y = test_data[1]

    # 计算训练误差
    train_y_ = model(w_params, train_x)
    train_err = rmse(train_y_, train_y)
    print("train_err = %.8f" % train_err)

    # 计算泛化误差
    test_y_ = model(w_params, test_x)
    test_err = rmse(test_y_, test_y)
    print("test_err = %.8f" % test_err)

    x, y = merge_data(data_feed)
    y_ = model(w_params, x)
    plt.plot(y_)
    plt.plot(y)
    plt.legend(["预测值", "实际值"])
    plt.show()


if __name__ == '__main__':
    data_feed = read_data()
    scatter(data_feed)
    w_params = get_params(data_feed["train_data"])
    validate_model(w_params, data_feed)
```

<img src="img\3.png" alt="3" style="zoom:80%;" /><img src="img\4.png" alt="4" style="zoom:80%;" />





# 三、决策树

## 3.1、基本概念
决策树用于处理分类问题，给定训练数据集，构建一颗决策树，并使用这个树对未知数据进行预测。

![5](img\5.png)

上图所示表示一个女孩是否去相亲的决策。通过给定的样本数据集构建了决策树，当输入x=（年龄=20,外貌=中等，收入=低）时，可知结果是不见。
**橘色节点：叶子节点，表示最终的结果**
**绿色节点：决策节点，根据某个特征进行判断**。

图中，首先以年龄作为根节点开始判断，然后是长相、收入、公务员。能否以长相作为根节点（或其他特征）？
一般，决策树是以**最易选择**的特征从上至下。本例中基于实际情况，首先看年龄，年龄条件不满足则直接不见，即年龄对于是否去相亲影响的因素很大。即是最易选择。
最易选择可理解成在作决策的时候纠结程度小的（或**混乱程度、不确定性程度**，容易选择的混乱程度小，不容易选择的混乱程度大），首先选择容易选择的特征作为决策节点，根据决策结果，导出新的问题，然后在重复以此类推，直到不能选择为止(不能选择即，所有待选项其混乱程度一致)

### 3.1.1、信息熵

前面提到**最易选择**，如何定量描述其程度，即混乱程度？

**信息量:**

信息量指信息多少的度量，如果某个事件发生的概率越小，则其信息量就越大。
如：<u>人群中，某个人年龄超过30</u>的信息量要远小于<u>人群中，某个人是公务员</u>的信息量

设有随机变量$X=\{x_1,x_2,,x_i,,x_n\}$，则x_i的信息量表示为：
$$
I(x_i)=-log(P(x_i)) \quad \quad (log底数为2，单位比特)
$$
随机变量X的信息量为：
$$
I(X)=-\sum_{i=1}^n log(P(x_i))
$$

**信息熵:**

定义事件x_i的不确定性程度为H(x_i)，则:
$$
H(x_i)=P(x_i)I(x_i)=-P(x_i)log(P(x_i))
$$
H(x):
$$
H(x)=-\sum_{i=1}^n (P(x_i)log(P(x_i)))
$$
H(x)称为X的信息熵，表示不确定性大小，值越大，不确定性越大，即越混乱。



**条件熵:**

表示在满足条件s下X的信息熵
$$
H(X|s)=P(s_0)H(X|s=s_0)
$$

**信息增益：**
设给定X数据下的信息熵（经验熵）为H(X),在特征s下X的条件熵为H(X|s)，则：
$$
g(X,s) = H(X)-H(X|s)
$$
表示，在s条件下，原始的熵下降了多少。

### 3.1.2、决策树算法流程
决策树结构上包含一个根节点，若干个决策节点和叶子节点。每个节点的输入都是一个数据集合（根节点输入的集合是训练样本全集），输出为训练样本的一个子集（叶子节点除外，叶子节点输出为结果标签）。
<img src="img\6.png" alt="6" style="zoom:90%;" />

**算法流程：**

设样本输入空间X有N个特征，其特征名称组成的数组为featureNames

步骤1：三种情况递归结束

判断样本集是否属于同一个标签A，如果是则当前节点标记为叶节点，并返回类别为A标签

或者

判断样本在所有特征条件下的信息熵相同，则标记当前节点为叶节点，并返回类别最多的那个


步骤2: 根据信息增益确定当前节点最优的决策特征（获取最容易选择的特征，作为节点的决策条件）

步骤3：根据第二步得到的最优特征F，获取特征F的取值集合

步骤4：遍历步骤3中得到集合，从样本集中取特征等于当前循环变量的样本子集，如果该子集为空，生成子节点，并标记为叶子节点，设置类别为样本集合中最多的那个。如果不是空集则重复步骤1。

伪代码：

```java
  public void train(final List<TrainData> trainDataSet, final List<String> featureNames, Node node) {
        //如果数据集中标签全部一样，则直接标记为叶节点，并设置标签值
        //或者trainDataSet中各特征的条件熵一致则为叶节点，标签值为类别最多的
        if (isSameConditionalEntropy(trainDataSet) || isSameClass(trainDataSet)) {
            Object y = getYByMax(trainDataSet);
            node.setLabel(y);
            return;
        }
        //从trainDataSet中选择最优的特征(featureNames范围内)作为当前节点的决策条件
        String featureName = optimizeFeature(trainDataSet, featureNames);
        node.setFeatureName(featureName);
        //在trainDataSet上取featureName的所有值（去重）
        List<Object> featureObjects = trainDataSet.stream()
                .map(trainData -> trainData.getX().get(node.getFeatureName()))
                .distinct()
                .collect(Collectors.toList());
        //循环特征featureName中的每一个值
        for (Object featureObject : featureObjects) {
            node.addDecisionFunction(o -> Objects.equals(o, featureObject));
            Node childNode = new Node();
            node.addChild(childNode);

            //从trainDataSet中取特征等于featureObject的样本子集
            List<TrainData> subsetTrainDataSet = getTrainDataSetByFeature(trainDataSet, node.getFeatureName(), featureObject);
            //如果是空集，则标记为叶子节点,其标签值设置为父节点中样本最多的类别
            if (subsetTrainDataSet == null || subsetTrainDataSet.isEmpty()) {
                Object y = getYByMax(trainDataSet);
                childNode.setLabel(y);
            } else {
                //不是空集，则递归
                train(subsetTrainDataSet, featureNames, childNode);
            }
        }
    }
```



## 3.2、最优决策特征的划分

设数据集$D = \{D_1,D_2,...,D_i,...Dn\}$,其中
$$
D_i = (\{x_{i1},x_{i2},x_{i3},...,x_{id}\},\{y_i\}) \quad (x_{ij} \quad y_i 均为离散值,y_i \in\{k_1,k_2,k_3,,k_i,,k_m\},表示有M个类别)
$$

若$P(k_i)$表示，k_i类别在整个数据集中所占的比例，则数据集D的原始熵(经验熵)为：
$$
H(Y) = -[P(k_1)log(P(k_1))+P(k_2)log(P(k_2))+...+P(k_m)log(P(k_m))]=-\sum_{i=1}^mP(k_m)log(P(k_m))
$$

令特征$x_{i1} \in \{s_1,s_2,,s_i,,s_n\}$,则特征x_i1下数据集D的信息熵为：
$$
H(Y|x_{i1}) = P(s_1)H(Y|x_{i1}={s_1})+P(s_2)H(Y|x_{i1}={s_2})+P(s_3)H(Y|x_{i1}={s_3})+...+P(s_n)H(Y|x_{i1}={s_n})
$$
$$
H(Y|x_{i1})=\sum_{i=1}^n P(s_i)H(Y|x_{i1}={s_i})
$$

令特征$x_{i2} \in \{s_1,s_2,,s_i,,s_n\}$,则特征x_i2下数据集D的信息熵为：
$$
H(Y|x_{i2}) = P(s_1)H(Y|x_{i2}={s_1})+P(s_2)H(Y|x_{i2}={s_2})+P(s_3)H(Y|x_{i2}={s_3})+...+P(s_n)H(Y|x_{i2}={s_n})
$$
$$
H(Y|x_{i2})=\sum_{i=1}^n P(s_i)H(Y|x_{i2}={s_i})
$$

同理，特征$x_{ij} \in \{s_1,s_2,,s_i,,s_n\}$,则特征x_{ij}下数据集D的信息熵为：
$$
H(Y|x_{ij}) =\sum_{i=1}^n P(s_i)H(Y|x_{ij}={s_i})
$$

原始信息熵与在特征$x_{ij} 条件下的信息熵之差表示为该特征的信息增益：
$$
g(Y,X) = H(Y)-H(Y|x_{ij})
$$

前面提到**最易选择的特征**，用熵表示就是选择该特征作划分后，其数据集的原始熵能够得到有效下降。即**最优的特征选择就是信息增益最大的那个特征**。



## 3.3、实例分析

### 3.3.1、划分选择

给定数据集如下，根据西瓜色泽 、根蒂 、敲声 、纹理 、脐部 、触感等特征，判断是否是好瓜
|编号|色泽|根蒂|敲声|纹理|脐部|触感|好瓜|
|  ----  | ----  | ----  | ----  | ----  | ----  | ----  | ----  |
|1|青绿|蜷缩|浊响|清晰|凹陷|硬滑|	是|
|2|乌黑|蜷缩|沉闷|清晰|凹陷|硬滑|	是|
|3|乌黑|蜷缩|虫响|清晰|凹陷|硬滑|	是|
|4|青绿|蜷缩|沉闷|清晰|凹陷|硬滑|	是|
|5|浅白|蜷缩|浊响|清晰|凹陷|硬滑|	是|
|6|青绿|稍蜷|浊响|清晰|稍凹|软粘|	是|
|7|乌黑|稍蜷|浊响|稍糊|稍凹|软粘|	是|
|8|乌黑|稍蜷|浊响|清晰|稍凹|硬滑|	是|
|9|乌黑|稍蜷|祝闷|稍糊|稍凹|硬滑|	否|
|10|青绿|硬挺|清脆|清晰|平坦|软粘|	否|
|11|浅白|硬挺|清脆|模糊|平坦|硬滑|	否|
|12|浅白|蜷缩|浊响|模糊|平坦|软粘|	否|
|13|青绿|稍蜷|浊响|稍糊|凹陷|硬滑|	否|
|14|浅白|稍蜷|沉闷|稍糊|凹陷|硬滑|	否|
|15|乌黑|稍蜷|浊响|清晰|稍凹|软粘|	否|
|16|浅白|蜷缩|浊响|模糊|平坦|硬滑|	否|
|17|青绿|蜷缩|沉闷|稍糊|稍凹|硬滑|	否|

* 计算数据集原始熵

$$
H(Y) = -[P(是)log(P(是))+P(否)log(P(否))]=-[\frac {8} {17}log(\frac {8} {17})+\frac {9} {17}log(\frac {9} {17})]=0.998
$$

* 分别计算色泽 、根蒂 、敲声 、纹理 、脐部 、触感等特征下数据集的条件熵

色泽：
$D_{色泽=青绿}  \in \{1,4,6,10,13,17\}$
$D_{色泽=乌黑}  \in \{2,3,7,8,9,15\}$
$D_{色泽=浅白}  \in \{5,11,12,14,16\}$
$$
H(Y|色泽)=P(青绿)H(Y|色泽=青绿)+P(乌黑)H(Y|色泽=乌黑)+P(浅白)H(Y|色泽=浅白)
$$
$$
H(Y|色泽)=-\frac {6} {17}(\frac {3} {6}log\frac {3} {6}+\frac {3} {6}log\frac {3} {6})-\frac {6} {17}(\frac {4} {6}log\frac {4} {6}+\frac {2} {6}log\frac {2} {6})-\frac {5} {17}(\frac {1} {5}log\frac {1} {5}+\frac {4} {5}log\frac {4} {5})=0.889
$$

同理有:
$$
H(Y|根蒂)=0.855
$$
$$
H(Y|敲声)=0.857
$$
$$
H(Y|纹理)=0.617
$$
$$
H(Y|脐部)=0.709
$$
$$
H(Y|触感)=0.992
$$

* 计算各特征信息增益，选取信息增益最大的作为特征划分
$$
G(Y,X)=H(Y)-H(Y|X)
$$
显然，$H(Y|纹理)=0.617$,最小，则$G(Y,纹理)=H(Y)-H(Y|纹理)=0.381$最大，所以，选取纹理作为第一个节点的划分特征。

选取纹理划分后，在训练数据集上得出三个子集：
$D_{纹理=清晰}  \in \{1,2,3,4,5,6,8,10,15\}$
$D_{纹理=稍糊}  \in \{7,9,13,14,17\}$
$D_{纹理=模糊}  \in \{11,12,16\}$



根据这三个子集，分别在各种分支节点上在重复上述操作，直到满足算法流程中递归结束条件为止。

### 3.3.2、代码实现

```java
public static void main(String[] args) {
        List<TrainData> trainDataSet = DecisionTree.loadTrainDataSet();
        List<String> featureNames = DecisionTree.featureNames();
        DecisionTree decisionTree = new DecisionTree();
        Node rootNode = new Node();
        decisionTree.train(trainDataSet, featureNames, rootNode);

        for (TrainData trainData : trainDataSet) {
            InferResult inferResult = new InferResult();
            Map<String, Object> x = trainData.getX();
            decisionTree.infer(rootNode, x, inferResult);
            System.out.printf("输入:%s,结果:%s-----期望:%s%n",x.values(), inferResult, trainData.getY());
        }
    }
```

DecisionTree.java

```java
@Data
public class DecisionTree {

    public void train(final List<TrainData> trainDataSet, final List<String> featureNames, Node node) {
        //如果数据集中标签全部一样，则直接标记为叶节点，并设置标签值
        //或者trainDataSet中各特征的条件熵一致则为叶节点，标签值为类别最多的
        if (isSameConditionalEntropy(trainDataSet) || isSameClass(trainDataSet)) {
            Object y = getYByMax(trainDataSet);
            node.setLabel(y);
            return;
        }
        //从trainDataSet中选择最优的特征(featureNames范围内)作为当前节点的决策条件
        String featureName = optimizeFeature(trainDataSet, featureNames);
        node.setFeatureName(featureName);
        //在trainDataSet上取featureName的所有值（去重）
        List<Object> featureObjects = trainDataSet.stream()
                .map(trainData -> trainData.getX().get(node.getFeatureName()))
                .distinct()
                .collect(Collectors.toList());
        //循环特征featureName中的每一个值
        for (Object featureObject : featureObjects) {
            node.addDecisionFunction(o -> Objects.equals(o, featureObject));
            Node childNode = new Node();
            node.addChild(childNode);

            //从trainDataSet中取特征等于featureObject的样本子集
            List<TrainData> subsetTrainDataSet = getTrainDataSetByFeature(trainDataSet, node.getFeatureName(), featureObject);
            //如果是空集，则标记为叶子节点,其标签值设置为父节点中样本最多的类别
            if (subsetTrainDataSet == null || subsetTrainDataSet.isEmpty()) {
                Object y = getYByMax(trainDataSet);
                childNode.setLabel(y);
            } else {
                //不是空集，则递归
                train(subsetTrainDataSet, featureNames, childNode);
            }
        }
    }

    public void infer(Node node, Map<String, Object> x, final InferResult inferResult) {
        if (node == null) {
            return;
        }
        if (node.isLeafNode()) {
            inferResult.setValue(node.getLabel());
            return;
        }
        Node childNode = node.decision(x);
        infer(childNode, x, inferResult);
    }

    /**
     * 满足feature = featureObject 的样本子集
     *
     * @param trainDataSet
     * @param featureName
     * @param featureObject
     * @return
     */
    private List<TrainData> getTrainDataSetByFeature(List<TrainData> trainDataSet, String featureName, Object featureObject) {
        return trainDataSet.stream().filter(trainData -> {
            Map<String, Object> x = trainData.getX();
            Object o = x.get(featureName);
            return Objects.equals(o, featureObject);
        }).collect(Collectors.toList());
    }
    
    /**
     * 选出最优的特征
     *
     * @param trainDataSet
     * @param featureNames
     * @return
     */
    private String optimizeFeature(List<TrainData> trainDataSet, List<String> featureNames) {
        Map<String, Double> map = featureNames.stream().collect(Collectors.toMap(s -> s, s -> {
            BigDecimal bigDecimal = conditionalEntropy(trainDataSet, s);
            return bigDecimal.doubleValue();
        }));
        List<Map.Entry<String, Double>> list = new ArrayList<>(map.entrySet());
        list.sort(Map.Entry.comparingByValue());
        return list.get(0).getKey();
    }
    
    /**
     * 获取标签最多的值
     *
     * @param trainDataSet
     * @return
     */
    private Object getYByMax(List<TrainData> trainDataSet) {
        List<Object> yCollect = trainDataSet.stream().map(TrainData::getY).collect(Collectors.toList());
        Map<Object, List<Object>> map = yCollect.stream().collect(Collectors.groupingBy(o -> o));
        Map<Object, Integer> numMap = map.entrySet().stream().collect(Collectors.toMap(Map.Entry::getKey, e -> e.getValue().size()));
        Integer max = Collections.max(numMap.values());
        final Object[] result = new Object[1];
        numMap.forEach((o, integer) -> {
            if (Objects.equals(integer, max)) {
                result[0] = o;
            }
        });
        return result[0];
    }

    /**
     * trainDataSet 信息熵
     *
     * @param trainDataSet
     * @return
     */
    public BigDecimal entropy(List<TrainData> trainDataSet) {
        if (trainDataSet.isEmpty()) {
            throw new RuntimeException("trainDataSet can not empty");
        }
        int size = trainDataSet.size();
        //统计类别
        List<Object> yCollect = trainDataSet.stream().map(TrainData::getY).collect(Collectors.toList());
        //去重
        List<Object> yCollectDistinct = yCollect.stream().distinct().collect(Collectors.toList());
        BigDecimal v = yCollectDistinct.stream().map(y -> {
            long count = yCollect.stream().filter(o -> Objects.equals(o, y)).count();
            BigDecimal p = new BigDecimal(count).divide(new BigDecimal(size), 8, BigDecimal.ROUND_HALF_UP);
            //信息量I=logP(Y)(2为底数)
            double i = Math.log(p.doubleValue()) / Math.log(2.0F);
            return p.multiply(new BigDecimal(i));
        }).reduce(BigDecimal.ZERO, BigDecimal::add);
        return v.multiply(BigDecimal.valueOf(-1.0F));
    }

    /**
     * featureName条件下trainDataSet的信息熵
     *
     * @param trainDataSet
     * @param featureName
     * @return
     */
    private BigDecimal conditionalEntropy(List<TrainData> trainDataSet, String featureName) {
        if (trainDataSet.isEmpty()) {
            throw new RuntimeException("trainDataSet can not empty");
        }
        int size = trainDataSet.size();
        List<Object> featureObjects = trainDataSet.stream()
                .map(trainData -> trainData.getX().get(featureName))
                .collect(Collectors.toList());
        List<Object> featureObjectsDistinct = featureObjects.stream().distinct().collect(Collectors.toList());
        return featureObjectsDistinct.stream().map(s -> {
            long count = featureObjects.stream().filter(o -> Objects.equals(o, s)).count();
            //P(X=s)
            BigDecimal p = new BigDecimal(count).divide(new BigDecimal(size), 8, BigDecimal.ROUND_HALF_UP);
            //计算H(Y|x=s)
            BigDecimal entropy = entropy(getTrainDataSetByFeature(trainDataSet, featureName, s));
            return p.multiply(entropy);
        }).reduce(BigDecimal.ZERO, BigDecimal::add);
    }

    /**
     * 所有特征条件下的信息熵是否一致
     *
     * @param trainDataSet
     * @return
     */
    private boolean isSameConditionalEntropy(List<TrainData> trainDataSet) {
        List<String> featureNames = DecisionTree.featureNames();
        List<BigDecimal> collect = featureNames.stream().map(f -> conditionalEntropy(trainDataSet, f))
                .distinct().collect(Collectors.toList());
        BigDecimal sum = collect.stream().reduce(BigDecimal.ZERO, BigDecimal::add);
        BigDecimal e = sum.divide(new BigDecimal(collect.size()), 8, BigDecimal.ROUND_HALF_UP);
        BigDecimal s = collect.stream().map(v -> {
                    BigDecimal subtract = v.subtract(e);
                    return subtract.multiply(subtract);
                }).reduce(BigDecimal.ZERO, BigDecimal::add)
                .divide(new BigDecimal(collect.size()), 8, BigDecimal.ROUND_HALF_UP);
        double dev = Math.sqrt(s.doubleValue());
        return dev < 0.0000001f;
    }

    private boolean isSameClass(List<TrainData> trainDataSet) {
        return trainDataSet.stream().map(TrainData::getY).distinct().count() == 1;
    }

    public static List<String> featureNames() {
        return Arrays.stream("色泽\t根蒂\t敲声\t纹理\t脐部\t触感".split("\t"))
                .collect(Collectors.toList());
    }

    public static List<TrainData> loadTrainDataSet() {
        String text = "青绿\t蜷缩\t浊响\t清晰\t凹陷\t硬滑\t是\n" +
                "乌黑\t蜷缩\t沉闷\t清晰\t凹陷\t硬滑\t是\n" +
                "乌黑\t蜷缩\t浊响\t清晰\t凹陷\t硬滑\t是\n" +
                "青绿\t蜷缩\t沉闷\t清晰\t凹陷\t硬滑\t是\n" +
                "浅白\t蜷缩\t浊响\t清晰\t凹陷\t硬滑\t是\n" +
                "青绿\t稍蜷\t浊响\t清晰\t稍凹\t软粘\t是\n" +
                "乌黑\t稍蜷\t浊响\t稍糊\t稍凹\t软粘\t是\n" +
                "乌黑\t稍蜷\t浊响\t清晰\t稍凹\t硬滑\t是\n" +
                "乌黑\t稍蜷\t沉闷\t稍糊\t稍凹\t硬滑\t否\n" +
                "青绿\t硬挺\t清脆\t清晰\t平坦\t软粘\t否\n" +
                "浅白\t硬挺\t清脆\t模糊\t平坦\t硬滑\t否\n" +
                "浅白\t蜷缩\t浊响\t模糊\t平坦\t软粘\t否\n" +
                "青绿\t稍蜷\t浊响\t稍糊\t凹陷\t硬滑\t否\n" +
                "浅白\t稍蜷\t沉闷\t稍糊\t凹陷\t硬滑\t否\n" +
                "乌黑\t稍蜷\t浊响\t清晰\t稍凹\t软粘\t否\n" +
                "浅白\t蜷缩\t浊响\t模糊\t平坦\t硬滑\t否\n" +
                "青绿\t蜷缩\t沉闷\t稍糊\t稍凹\t硬滑\t否";
        List<String> featureNames = DecisionTree.featureNames();
        return Arrays.stream(text.split("\n")).map(s -> {
            TrainData trainData = new TrainData();
            String[] split = s.split("\t");
            Map<String, Object> x = new HashMap<>();
            for (int i = 0; i < featureNames.size(); i++) {
                x.put(featureNames.get(i), split[i]);
            }
            trainData.setY(split[split.length - 1]);
            trainData.setX(x);
            return trainData;
        }).collect(Collectors.toList());
    }

}
```

Node.java

```java
@Data
public class Node {
    private Node parentNode;
    private final List<Node> childNodes;
    private String featureName;
    private final List<Function<Object, Boolean>> decisionFunctions;
    private Object label;

    public Node() {
        this.childNodes = new ArrayList<>();
        this.decisionFunctions = new ArrayList<>();

    }

    public void addChild(Node node) {
        node.setParentNode(this);
        childNodes.add(node);

    }

    public void addDecisionFunction(Function<Object, Boolean> function) {
        decisionFunctions.add(function);
    }


    public Boolean isLeafNode() {
        return childNodes.isEmpty();
    }

    public Object getLabel() {
        if (isLeafNode()) {
            return label;
        }
        throw new RuntimeException("Only leaf nodes can be called");
    }

    public Node decision(Map<String, Object> x) {
        if (isLeafNode()) {
            return null;
        }
        Object object = x.get(featureName);
        if (!decisionFunctions.isEmpty()) {
            for (int i = 0; i < decisionFunctions.size(); i++) {
                Boolean decisionResult = decisionFunctions.get(i).apply(object);
                if (decisionResult) {
                    return childNodes.get(i);
                }
            }
        }
        throw new RuntimeException("node exception");
    }
}

```



InferResult.java

```java
@Data
public class InferResult {
    private Object value;
    @Override
    public String toString() {
        return value.toString();
    }
}
```



TrainData.java

```java
@Data
public class TrainData {
    private Map<String, Object> X;
    private Object Y;
}
```

## 3.4、剪枝处理

决策树种随着节点数量的增加，其泛化误差也会越来越大，这是由于过拟合原因产生的。剪枝处理可有效应对过拟合问题。

![7](img\7.png)


## 3.4.1、预剪枝

将数据集划分两部分，一部分训练集，一部分验证集。使用训练集基于信息熵的方式导出决策树，对于每一个节点，先尝试另其为叶子节点（其类别标记为训练样例数最多的类别），并计算在验证集上的精度，然后对该节点进行划分，并重新计算在验证集上的精度，若划分前精度大于划分后，则对于该节点则停止划分，否则继续划分。

## 3.4.2、后剪枝

后剪枝是先从训练集中生成一个的完整决策树，然后自低向上，逐一考察非叶子节点。先剪掉其领衔的叶子节点，并计算在验证集上的精度，如精度有提高则实施剪枝策略（如果精度没有变化，则根据奥卡姆剃刀原则同样实施剪枝策略）。

后剪枝比预剪枝保留了更多的节点，其欠拟合的风险也比较小。

## 3.5、连续值处理

决策树对于连续值的特征进行处理的方法主要是将连续性数值特征划分为不同的区间，从而变成离散的数值。C4.5中采用了二分法对连续值进行处理。

二分法的思想为:对于某个属性出现的**连续值从小到大排列**，取**每两个点的中点进行划分**（任取两点之间的任意一点不影响划分结果），这样能够获取到n-1（n为样本数）个划分点。选取其中信息增益最大的划分点作为最终划分节点的依据。

> 参考：https://blog.csdn.net/u012328159/article/details/79396893



# 四、支持向量机


## 4.1 二分类问题

给定数据集$D = \{D_1,D_2,...,D_i,...Dn\}$,其中
$$
D_i = (\{x_{i1},x_{i2},x_{i3},...,x_{id}\},\{y_i\}) \quad \quad y_i \in\{-1,1\}
$$

样本空间D中有两个类别(类别-1和1)，二分类的基本思想是找到一个超平面(特征x的维度是1时，是一条直线，维度2是一个平面，维度大于2时是超平面)将样本空间划分成两部分，将这两个类别区分出来。

<img src="img\8.png" alt="8" style="zoom:75%;" />

如上图所示，圆圈表示类别1，矩形表示类别-1，中间黑色线条表示一个超平面，将两个类别区分开来。这个超平面需满足如下条件：
$$
\begin{cases}
\pmb{\omega}^T\pmb{x_i}  + b \ge 0 \qquad & y_i = 1\\
\pmb{\omega}^T\pmb{x_i}  + b \lt 0 \qquad & y_i = -1
\end{cases}
$$

即当x属于类别1时,$\pmb{\omega}^T\pmb{x_i}  + b \ge 0$ ，当x属于类别-1时，$\pmb{\omega}^T\pmb{x_i}  + b \lt 0$。将上面不等式组变换后，可等价表示成：
$$
y_i(\pmb{\omega}^T\pmb{x_i}  + b) \ge 0  \quad \quad y_i \in\{-1,1\}
$$
只要满足上式条件的超平面，可将样本正确划分成两类。显然对于同一数据集这样的超平面有无限多个。

## 4.2 间隔(Margin)

前面提到将两类样本完全区分开的超平面有无数个，如下图所示（仅画出三个）。

<img src="img\9.png" alt="9" style="zoom:95%;" />

其中α、λ均离两类样本相近，当样本中引入随机噪声时，离超平面相近的样本点很容易划分到错误的类别。
很明显超平面β的划分效果是最好的，他离两类样本点的距离都最远，即预测时对未见样本的泛化能力最强。

如下图所示，设有超平面α 、λ，令
**α：$\pmb{\omega}^T\pmb{x_i}  + b = c$** 
**λ：$\pmb{\omega}^T\pmb{x_i}  + b = -c$**

> c是一个常数，且c>0

<img src="img\10.png" alt="10" style="zoom:95%;" />

样本$x_1,x_2$属于类别1，且在超平面α上;样本$x_3,x_4$属于类别-1，且在超平面λ上;则**间隔(Margin)表示α到λ的距离d**。
$x_1,x_2,x_3,x_4$称为**支持向量（support vector）**。

当找到合适的α，λ使得**间隔最大化**时，对应的解析式（最大化时对应的参数w、b）：
$$
\pmb{\omega}^T\pmb{x_i}  + b=0
$$
就是最优的划分超平面。

间隔d表示：
由图可知，d可表示成样本x2到x3的距离:

$$
d=|\vec{x_2} - \vec{x_3}|=k|\vec{\omega}|=k||\omega||  \quad  
(k是实数)
$$
>$ \omega 是超平面的法向量,与向量\vec{x_2} - \vec{x_3}是平行的，所以可以表示成一个实数与法向量的乘积$

因为
$$
=> x_2=x_3+k\omega 
$$
将$x_2$带入α 则有：$ α:\omega^T(x_3+k\omega)  + b = c$ 展开后：
$$
\omega^Tx_3+b+k\omega^T\omega=c
$$

$x_3$在λ上，所以有$\omega^Tx_3+b=-c$,上式变为：
$$
k\omega^T\omega = 2c    \quad  =>  k= \frac{2c}{\omega^T\omega}   
$$
所以有：
$$
d=k||\omega||= ||\omega||\frac{2c}{\omega^T\omega}
$$
$$
d=||\omega||\frac{2c}{||\omega||^2}=\frac{2c}{||\omega||}
$$

间隔d的另外一种推导方式：
间隔d也表示成$x_2$到超平面λ的距离，根据距离公式有：
$$
d=\frac{|\omega^Tx_2+b+c|}{||\omega||}=\frac{2c}{||\omega||}
$$


## 4.3 目标函数

在条件：
$$
\begin{cases}
\pmb{\omega}^T\pmb{x_i}  + b \ge c \qquad & y_i = 1 \qquad(表示x_i均在α超平面上端)\\
\pmb{\omega}^T\pmb{x_i}  + b \lt -c \qquad & y_i = -1 \qquad(表示x_i均在λ超平面下端)
\end{cases}
$$
下，最大化间隔d:
$$
\underset{\omega,b}{max}(\frac{2c}{||\pmb{\omega}||})
$$

由于c是常数，替换成1变形得：
$$
\begin{cases}
\underset{\omega,b}{max}  \qquad \frac{2}{||\pmb{\omega}||} \qquad (优化函数)\\
y_i(\pmb{\omega}^T\pmb{x_i}  + b) \ge 1 \qquad (约束条件)
\end{cases}
$$
=>
$$
\begin{cases}
\underset{\omega,b}{min}  \qquad  \frac{1}{2}||\pmb{\omega}||^2  \\ 
y_i(\pmb{\omega}^T\pmb{x_i}  + b) \ge 1 \qquad 
\end{cases}
$$
> 平方与系数1/2的引入是为了方便求导。对原始优化问题没有影响。

## 4.3 软间隔目标函数

前面的推导是基于样本在空间中完全线性可分，即存在一个超平面能够完全将两类样本正确划分，但在实际问题中，会存在线性不可分情况。如下图所示:

<img src="img\11.png" alt="11" style="zoom:90%;" />

图中是无法找到一个超平面完全将样本正确划分。完全将样本正确划分的超平面(满足条件:$y_i(\pmb{\omega}^T\pmb{x_i}  + b) \ge 1$)即然找不到，是不是对于线性不可分的问题就无解？当然不是，软间隔概念的引入即是为了解决线性不可分问题。

软间隔的思想是允许部分样本分类错误，但又希望总的错误尽量小。则优化问题变为：
$$
\begin{cases}
\underset{\omega,b}{min}  \qquad \frac{1}{2}||\pmb{\omega}||^2 + k \sum_{i=1}^n e_i \qquad  (e_i \ge 0)\\
y_i(\pmb{\omega}^T\pmb{x_i}  + b) \ge 1-e_i \qquad 
\end{cases}
$$

> k是一个常数，可以理解成表示的是一个误差项的权重，$e_i$表示样本$x_i$不满足强硬约束条件的程度, n表示样本个数。

软间隔将之前的强硬条件$y_i(\pmb{\omega}^T\pmb{x_i}  + b) \ge 1$ ，放宽为$y_i(\pmb{\omega}^T\pmb{x_i}  + b) \ge 1-e_i$ ,即允许了不部分样本点不满足之前的强硬条件。

对约束条件变形得：
$$
e_i\ge 1- y_i(\pmb{\omega}^T\pmb{x_i}  + b)\qquad
$$
因为$e_i$ 是非负数，当等于0得时候表示$y_i(\pmb{\omega}^T\pmb{x_i}  + b)=1$,即就是满足强硬条件（硬间隔）,此时优化目标就是最小化$\qquad ||\pmb{\omega}||^2$
当大于0时，即$y_i(\pmb{\omega}^T\pmb{x_i}  + b)<1$,不满足强硬条件，此时优化目标是最小化$||\pmb{\omega}||^2 + k \sum_{i=1}^n e_i$。

所以综上所述，目标函数(hinge loss)可表示为:
$$
\underset{\omega,b}{min}  \qquad \frac{1}{2}||\pmb{\omega}||^2 + k \sum_{i=1}^n max(0,1- y_i(\pmb{\omega}^T\pmb{x_i}  + b))
$$

## 4.4 拉格朗日对偶问题

SVM的目标函数（硬间隔），带有复杂的约束条件，计算时比较困难。为了方便计算，可以将原来的优化问题（原问题）转换成另一个问题（对偶问题）进行优化求解。
设原问题的最优解为D，对偶问题的最优解为Q，则对于任意原问题与其对偶问题都满足：
$$
D \le Q
$$
**对偶问题最优解是原问题最优解的下界。**
特殊的当原问题是一个凸优化问题时，有：
$$
D = Q
$$
称为强对偶性。在SVM中，优化问题是一个凸优化问题，所以SVM中对偶问题的最优解就是原问题的最优解。

### 4.4.1 KKT条件
对于带约束条件的优化问题，KKT条件是取得最优解的一个必要条件。即在可行域中找一点P，如果函数f(p)取得极值，则该点P一定是满足KKT条件的。如果优化问题是凸优化，那么KKT条件就是取得最优解的充要条件。

设有$x \in R^n $，优化目标函数f(x)
$$
\begin{cases}
\underset{x}{min}  f(x) \\ 
h_i(x)=0 \qquad i=1,2,3...k\\
g_j(x) \le 0  \qquad j=1,2,3...l\\
\end{cases}
$$
写成广义的拉格朗日函数为：
$$
L(x,α,β)=f(x)+\sum_{i=1}^kα_ih_i(x)+\sum_{i=1}^lβ_ig_i(x)
$$

若$x^*$是f(x)的最优解，则$x^*$满足：
$$
\begin{cases}
\frac{\partial L(x,α,β)}{\partial x}|_{x=x^*} = 0  \\ 
β_i \ge 0 \\
β_ig_i(x^*) \le0 \\
h_i(x^*)=0  \\
g_j(x^*) \le 0   \\
\end{cases}
$$
上式则为目标函数f(x)的KKT条件。

利用KKT条件优化实例:
$$
\underset{x}{min}f(x_1,x_2)=(x_1-3)^2-(x_2-1)^2 \\
st. \qquad x_1+x_2=0 \qquad x_1 \le 5
$$
写出拉格朗日函数：$L(x,α,β)=(x_1-3)^2+(x_2-1)^2+α(x_1+x_2)+β(x_1-5)$
写出KKT条件：
$$
\begin{cases}
\frac{\partial L(x,α,β)}{\partial x_1} =2(x_1-3)+α+β=0  \\ 
\frac{\partial L(x,α,β)}{\partial x_2} =2(x_2-1)+α=0  \\ 
β \ge 0 \\
β(x_1-5) \le0 \\
x_1 \le 5  \\
x_1+x_2=0   \\
\end{cases}
$$
=>
$$
\begin{cases}
2(x_1-3)+α+β=0  \\ 
2(x_2-1)+α=0  \\ 
β \ge 0 \\
β(x_1-5) \le0 \\
x_1 \le 5  \\
x_1+x_2=0   \\
\end{cases}
$$
=>
$$
\begin{cases}
2x_1-2x_2-8+β=0  \\ 
β \ge 0 \\
β(x_1-5) \le0 \\
x_1 \le 5  \\
x_1+x_2=0   \\
\end{cases}
$$

