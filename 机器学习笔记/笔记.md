[toc]



# 一、基本概念

## 1.1、数据集

### 1.1.1、定义

$$
D = \{D_1,D_2,...,D_i,...Dn\}
$$

$$
D_i = (\{x_{i1},x_{i2},x_{i3},...,x_{id}\},\{y_{i1},y_{i2},y_{i3},...,y_{im}\})=(\mathbf{x_i},\mathbf{y_i})
$$

在样本空间中采样N个示例(D~1~,D~2~,D~3~,...,D~n~)组成D，作为一个训练集。每个示例D~i~(或每个样本)由**x~i~**，**y~i~**两部分组成。
**x~i~**是一个d维向量，或称特征向量
**y~i~**是一个m维向量，或称标签

## 1.2、机器学习
### 1.2.1、定义

对某一任务，给定数据集D~train~，通过一种算法（学习算法），得到**x~i~**与**y~i~**的映射关系y=f(x)，映射关系**f**称为学习算法在数据集D上学习到的**经验(模型)**得到模型后并对未知数据D~test~进行预测。

> 机器学习形式化的定义
>
> 假设用P来评估计算机程序在某任务类T上的性能，若一个程序通过利用经验E在T中任务丰获得了性能改善，则我们就说关于T和P，该程序对E进行了学习

通过算法得到模型后，在另一数据集D~test~（D~test~、D~train~均来自同一样本空间采样）上进行预测的过程称为**测试**。我们希望模型能够很好适用于D~test~，这种在“新样本”上的适用能力称为**泛化**。

>具有强泛化能力的模型能很好地适用于整个样本空间.
>训练集应能能很好地反映出样本空间的特性.
>训练集中每个样本应独立地从样本空间分布上采样获得的，即独立同分布.
>训练样本越多，越能反映出样本空间特性，通过学习算法获得的模型强泛化能力越强



### 1.2.2、机器学习任务分类

根据训练数据集是否拥有标签信息,学习任务可分为：

**监督学习：**

$$
D_i = (\{x_{i1},x_{i2},x_{i3},...,x_{id}\},\{y_{i1},y_{i2},y_{i3},...,y_{im}\})=(\mathbf{x_i},\mathbf{y_i})
$$

**非监督学习：**
当数据集D中每个样本D~i~仅由x~i~组成，即不包含标签数据时
$$
D_i = \{x_{i1},x_{i2},x_{i3},...,x_{id}\}=\mathbf{x_i}
$$

根据模型预测输出值的情况，学习任务可分为：

**回归：**
输出值是连续的，如根据历史数据，预测未来十年全球人口数量

**分类:**
输出值是离散的，如根据邮件内容识别邮件是否是垃圾邮件（输出是一个逻辑状态，是或否，没有中间状态）




## 1.3、模型性能指标

### 1.3.1、 错误率
对于分类任务，如果有M个样本中，有a个样本分类错误，则模型的**误差率为a/m**。**精度为1-a/m**

### 1.3.2、误差

模型预测输出与真实输出的差异

### 1.3.3、经验误差

模型在训练集上的误差称为经验误差或训练误差

### 1.3.4、 泛化误差

模型在新样本上的误差称为**泛化误差**

**通常最终目的我们是希望模型的泛化误差越小越好**

在训练模型时为了得到泛化误差，以此来近似评价模型的好坏。通常准备两个数据集，一个是训练模型需要的训练集，另一个验证模型好坏的验证集。

训练集、验证集数据获取方法：

**留出法**：

将数据集D划分两个互斥集合D~train~(训练集) ，D~verify~（测试集）.  D~train~ ∩ D~verify~ = 空集,  D~train~ ∪ D~verify~ = D

在D~train~ 上训练出模型后，通过，D~verify~ 来评估模型的测试误差，作为泛化误差的估计。

 > 单次使用留出法得到的估计结果往往不够稳定可靠，在使用留出法时，一般要采用若干次随机划分、重复进行实验评估后取平均值作为留出法的评估结果.例如进行100次随机划分，每次产生一个训练/测试集用同时可得估计结呆的标于实验评估，100次后就得到100个结果?而留出法返回的则是这100个结果的平均.

**k折交叉验证法**:

将数据集D划分成K份，D= {D~1~，D~2~，...,D~k~}, 分别取其中一个子集作为测试集，其他剩余作为训练集。即将D分成了K组数据:

  > group~1~
  > D~train~= {D~1~，D~2~，...,D~k-1~}   D~verify~={D~k~}
  >
  > group~2~
  > D~train~= {D~1~，D~2~，...,D~k-2~} ∪ {D~k~}   D~verify~={D~k-1~}
  >
  > group~3~
  > D~train~= {D~1~，D~2~，...,D~k-3~} ∪ {D~k-1~,D~k~}   D~verify~={D~k-2~}
  >
  > ...
  >
  > group~k~
  > D~train~= {D~2~，...，D~k~}   D~verify~={D~1~}

获得k组训练/测试集后，可进行k次训练和测试，最终返回的是这k个测试结果的均值。

**自助法:**

在一个包含N个样本的数据集D中，每次随即抽取一个样本，并抽取N次，得到新的数据集D'.由于

每次采样可能采集到重复的样本，所以D中有部分样本没有出现在D‘中，假设每个样本不被抽中的概率为1-1/N，那么采样N次后，始终不被抽样中的概率为(1-1/N)^n^, 对N取极限则得到概率为
1/e  ≈ 0.368. 即有36.8%的样本没有出现在D’中。这样可以使用D‘作为训练集，D与D’的差集作为验证集。

  D~train~ = D‘

  D~verify~  = D - D’

自助法适用与当数据集比较少的情况。

  

**经验误差与泛化误差的关系：**

经验误差小，泛化误差可能会比较大（过拟合）

经验误差大，泛化误差可能大（欠拟合），可能小

**经验误差与泛化误差没有必然联系**

实际应在保证经验误差小（并不是越小越好）的情况下，减小泛化误差。

总之，最终目的是学得的模型应在样本空间中具有较强的泛化能力，即减小泛化误差

### 1.3.5、过拟合
学习算法过于强大，模型记住了训练样本中的**非普遍规律**，特点是经验误差很小，泛化误差大。 


<img src="img\1.png" alt="1" style="zoom:85%;" />

### 1.3.6、 欠拟合
学习算法过于弱小，模型不能表述样本普遍特性，特点是经验误差大，泛化误差大。

# 二、线性模型
## 2.1 线性模型一般形式
$$
y=f(x) = \omega_1 x_1+ \omega_2 x_2+ \omega_2 x_2+...+ \omega_d x_d+b
$$
或
$$
y=f(\pmb{x}) = \pmb{\omega}^T\pmb{x}  + b
$$

x（x~1~，x~2~，x~3~，...，x~d~）表示模型的输入，y表示输出。
ω（ω~1~，ω~2~，ω~3~，...，ω~d~）、b表示模型的参数，当ω、b参数确定后，模型也被确定下来。



## 2.2 线性回归

给定数据集$D=\{D_1,D_2,...,D_n\}$中某个样本 $D_i = (\{x_{i1},x_{i2},x_{i3},...,x_{id}\},\{y_i\})$假设可以使用线性模型来表示数据集中**x与 y的关系**。
$$
\pmb{y} =  \pmb{\omega}^T\pmb{x}  + b
$$

线性回归任务是通过某学习算法，找到合适的ω、b参数，使数据集D中任意样本D_i尽可能都满足以上公式。

### 2.2.1 单变量线性回归
考虑当输入特征仅只有一个的情况，即输入是一个标量。
设数据集$D =\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\}$
使用线性模型来表示数据集x、y的关系：
$$
y =f(x)= \omega x + b
$$
通过算法找到合适的ω、b参数，尽可能使得：
$$
y_i ≈ f(x_i) = \omega x_i + b
$$

使用均方误差来作为模型f的性能度量
$$
E = \sum_{i=1}^n (y'_i-y_i)^2 = \sum_{i=1}^n (\omega x_i + b-y_i)^2
$$
>y~i~表示数据集中样本标记，y'~i~表示模型f预测的实际输出。

目标函数E的优化过程：
对于函数E，当取得极小值时，对应的ω、b就是最优解。
对b求导：
$$
\frac{\partial E}{\partial b} = \frac{\partial \sum_{i=1}^n (\omega x_i + b-y_i)^2}{\partial b}
$$
$$
\frac{\partial E}{\partial b} = 2\sum_{i=1}^n (\omega x_i + b-y_i)
$$
令$\frac{\partial E}{\partial b}=0$，则：
$$
\sum_{i=1}^n (\omega x_i + b-y_i)=0
$$
$$
\sum_{i=1}^n \omega x_i + \sum_{i=1}^n b - \sum_{i=1}^n y_i=0
$$
$$
\omega \sum_{i=1}^n x_i + \sum_{i=1}^n b - \sum_{i=1}^n y_i=0
$$

对于随机变量a有 $\sum_{i=1}^n a_i = N\overline{a}$，$\overline{a} 表示a的平均数$，所以对上式表示为:
$$
\omega N\overline{x} + Nb = N\overline{y}
$$

**b的最优解**
$$
b = \overline{y} - \omega \overline{x}
$$

接下来对ω求导：
$$
\frac{\partial E}{\partial \omega} = \frac{\partial \sum_{i=1}^n (\omega x_i + b-y_i)^2}{\partial \omega}
$$
$$
\frac{\partial E}{\partial \omega} =   2\sum_{i=1}^n (\omega x_i + b-y_i)x_i
$$

令$\frac{\partial E}{\partial \omega}=0$，则：
$$
\sum_{i=1}^n (\omega x_i + b-y_i)x_i = 0
$$
$$
\sum_{i=1}^n \omega x_i^2 + \sum_{i=1}^n bx_i- \sum_{i=1}^n y_ix_i = 0
$$
$$
\omega N\overline{x^2} + bN\overline{x}- N\overline{xy} = 0
$$
将b的最优解带入上式:
$$
\omega N\overline{x^2} + (\overline{y} - \omega \overline{x})N \ \overline{x}- N\overline{xy} = 0
$$
$$
\omega \overline{x^2} + (\overline{y} - \omega \overline{x})\overline{x}- \overline{xy} = 0
$$
$$
\omega \overline{x^2}  - \omega \overline{x}^2 + \overline{x} \ \overline{y} - \overline{xy} = 0
$$
$$
\omega (\overline{x^2}  -  \overline{x}^2)  = \overline{xy} - \overline{x} \ \overline{y}
$$

**ω的最优解:**
$$
\omega =  \frac{\overline{xy} - \overline{x} \ \overline{y}}{\overline{x^2}  -  \overline{x}^2}
$$


### 2.2.2 多元线性回归

给定数据集$D = \{D_1,D_2,...,D_i,...Dn\} $,其中
$$
D_i = (\{x_{i1},x_{i2},x_{i3},...,x_{id}\},\{y_i\})
$$

使用线性模型来表示数据集x、y的关系：
$$
y=f(\pmb{x}) = \pmb{\omega}^T\pmb{x}  + b
$$

通过算法找到合适的ω、b参数，尽可能使得：
$$
y_i≈f(\pmb{x_i}) = \pmb{\omega}^T\pmb{x_i}  + b  \quad (\pmb{x_i}\in R^d,y_i \in R)
$$
模型的目标优化函数使用均方误差表示：
$$
E = \sum_{i=1}^n (y'_i-y_i)^2 = \sum_{i=1}^n (\pmb{\omega}^T\pmb{x_i}  + b -y_i)^2
$$
